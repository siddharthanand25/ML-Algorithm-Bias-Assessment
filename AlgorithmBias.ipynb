{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Bias\n",
    "###  Objective\n",
    "<span style=\"color:blue\">\n",
    "The objective of this assignment is to assess the impact of algorithm bias on machine learning algorithms trained with imbalanced data. Submissions should also propose and evaluate strategies for overcoming this bias. \n",
    "</span>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Load the Survival dataset and assess the bias of classifiers trained on this dataset, i.e. are classifiers biased towards the majority class. \n",
    "</span>\n",
    "\n",
    "I'll be comparing hold-out testing and cross-validation techniques to analyze the Bias generated by different Machine Learning Alogrithms k-NN, Decision Trees, Logistic Regression and Gradient Boosting.\n",
    "\n",
    "I'll be showing the count of Majority and Minority class after splitting the data into training and test sets, and thus will comparing how different ML models are predicting the count of minority classes. If the models are not predicitng accurately the count of miority classes then we can assure that there is Algorithm Bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "surv = pd.read_csv('survival.csv')   #loading the survival dataset as dataframe\n",
    "surv.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetcount=surv['Class'].value_counts()\n",
    "print('total count of class type 1 in the survival dataset:',targetcount[1])\n",
    "print('total count of class type 2 in the survival dataset:',targetcount[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = surv.drop('Class', axis=1)   #X will become the independent variable for the models.\n",
    "y = surv['Class']                #y will become the dependent vaiables.\n",
    "X.shape, y.shape                 # checking rows and columns in the X and y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minority class type 2 in the entire dataset(percentage wise): %0.2f\" % (Counter(y)[2]/len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Hold-Out Testing to Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.5,random_state=42)\n",
    "print('Actual Class type [1 and 2] feaures in test set: ',Counter(y_test))\n",
    "print('Minority Class Type [2] in test set: ',Counter(y_test)[2])\n",
    "test_neg = Counter(y_test)[2]\n",
    "Minority_test= test_neg/len(y_test)\n",
    "print(\"Minority class in test set percentage wise : %0.2f\" % (Minority_test))\n",
    "print('*' * 20)\n",
    "\n",
    "\n",
    "MLalgos ={}\n",
    "\n",
    "MLalgos['KNN'] = KNeighborsClassifier(n_neighbors=3)\n",
    "MLalgos['DecisionTree']= DecisionTreeClassifier(criterion='entropy')\n",
    "MLalgos['LogRegression'] = LogisticRegression()\n",
    "MLalgos['GradBoosting']= GradientBoostingClassifier()\n",
    "bias_calculated ={}\n",
    "accuracy_calculated={}\n",
    "\n",
    "\n",
    "for algo in MLalgos:\n",
    "    print(type(MLalgos[algo]).__name__)\n",
    "    y_predicted = MLalgos[algo].fit(X_train, y_train).predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, y_predicted)\n",
    "    print(\"Confusion matrix is :\\n{}\".format(confusion)) \n",
    "    acc = accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy:  %0.2f' % acc)\n",
    "    accuracy_calculated[algo]=acc\n",
    "    count_predicted = (y_predicted.sum()-len(y_predicted))\n",
    "    bias_calculated[algo]= count_predicted\n",
    "    print(\"Predicted minority class type 2 :\",count_predicted)\n",
    "    pred_neg = Counter(y_predicted)[2]\n",
    "    test_neg = Counter(y_test)[2]\n",
    "    print(\"Predicted minority class type 2 percentage wise : %0.2f\" % (pred_neg/len(y_predicted)))\n",
    "    predicted_count= pred_neg/len(y_predicted)\n",
    "    print('*' * 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Bias-Values using Matplotlib \n",
    "\n",
    "\n",
    "- Values in maroon are the values for minority class predicted by different modes \n",
    "- Value in yellow is the actual Minority Class value in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "algorithms = list(MLalgos.keys()) + ['Test-Set']\n",
    "predicted_values = list(bias_calculated.values()) + [test_neg]\n",
    "   \n",
    "fig = plt.figure(figsize = (8, 8)) \n",
    "  \n",
    "# creating the bar plot \n",
    "plt.bar(algorithms, predicted_values, color =['maroon','maroon','maroon','maroon','yellow']  ,\n",
    "        width = 0.5)   \n",
    "plt.xlabel(\"CLASSIFICATION MODELS\") \n",
    "plt.ylabel(\"TYPE 2 COUNT IN TEST\") \n",
    "plt.title(\"HOLD-OUT TESTING BIAS PLOT\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold Out Testing Outcomes\n",
    "- Splitted the training and testing set into two halves\n",
    "- Gradient Boosting is not showing Bias and is more accurate. \n",
    "- KNN, Decision Tree is showing showing Bias. 25 and 41 values predicted for the minority class when compared with 36 in test-set\n",
    "- Logistic Regression is showing huge Bias. 10 values are predicted only for the minority clas\n",
    "- The accuracy of the KNN and Logistic regression is more as compared to others but still there are large Bias values in these models. Hence we can say that, Accuracy does not count in showing the less Bias in the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Cross Validation to Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate       #importing the necessary libraries from SKLearn\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the confusion matrix and folds for the Cross validation techniques\n",
    "def TruePos(y_true, y_predicted):\n",
    "    return confusion_matrix(y_true, y_predicted)[1, 1]\n",
    "def TrueNeg(y_true, y_predicted): \n",
    "    return confusion_matrix(y_true, y_predicted)[0, 0]\n",
    "def FalsePos(y_true, y_predicted): \n",
    "    return confusion_matrix(y_true, y_predicted)[0, 1]\n",
    "def FalseNeg(y_true, y_predicted): \n",
    "    return confusion_matrix(y_true, y_predicted)[1, 0]\n",
    "scoring = {'tp' : make_scorer(TruePos), 'tn' : make_scorer(TrueNeg),\n",
    "           'fp' : make_scorer(FalsePos), 'fn' : make_scorer(FalseNeg)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kFolds = 22\n",
    "bias_calculated_CV = {}\n",
    "print('Minority Class Type [2] in cross-validation set:',targetcount[2] )\n",
    "print('*' * 20)\n",
    "\n",
    "for algo in MLalgos:\n",
    "     print(type(MLalgos[algo]).__name__)\n",
    "     cv_results = cross_validate(MLalgos[algo], X, y, cv= kFolds,scoring=scoring)\n",
    "     n_tot =  (cv_results['test_tp'].sum() +  cv_results['test_fp'].sum())   \n",
    "     acc_calculated_CV = (cv_results['test_tp'].sum() + cv_results['test_tn'].sum())/len(y)\n",
    "     print(\"Accuracy:{:.2f}\".format(acc_calculated_CV) )\n",
    "     print(\"{} x CV sets\".format(kFolds))   \n",
    "     bias_calculated_CV[algo] = n_tot\n",
    "     print(\"Predicted minority Class Type [2] in cross-validation set:\",n_tot )\n",
    "     print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = list(MLalgos.keys()) + ['Test-Set']\n",
    "predicted_values = list(bias_calculated_CV.values()) + [targetcount[2]]\n",
    "   \n",
    "fig = plt.figure(figsize = (6.5, 6.5)) \n",
    "  \n",
    "# creating the bar plot \n",
    "plt.bar(algorithms, predicted_values, color =['maroon','maroon','maroon','maroon','yellow']  ,\n",
    "        width = 0.5)   \n",
    "plt.xlabel(\"CLASSIFICATION MODELS\") \n",
    "plt.ylabel(\"TYPE 2 COUNT IN TEST\") \n",
    "plt.title(\"CROSS-VALIDATION BIAS PLOT\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Outcomes\n",
    "- This technique allows us to feed all of the minority class data i.e, 81 as compared to 36 in the Hold-Out testing. Hence we    can actually make some predictions with all of the data present.\n",
    "- Gradient Boosting shows a little Bias compared to other three algorithms.\n",
    "- Changing the value of KFolds can significantly change the bias count and accuracy score.\n",
    "- More accurate model such as Logistic Regression in this case should generate less Bias, but as oppose to that it is generating more Bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Proposing a strategy to rectify this bias. Evaluating the effect of this strategy in terms of classification bias and overall accuracy. \n",
    "</span>\n",
    "\n",
    "- Using Synthetic Minority Over-sampling Technique and utilizing it to upsample the minority class that is type 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = surv.drop('Class', axis=1)\n",
    "y = surv['Class']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=20, sampling_strategy = 0.7)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train), len(y_train_res)\n",
    "y_train.sum(), y_train_res.sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Majority class in previous training technique with hold-out testing:\",Counter(y_train)[1])\n",
    "print(\"Minority class in previous training technique with hold-out testing:\", Counter(y_train)[2])\n",
    "print(\"Majority class in SMOTE technique with hold-out testing:\",Counter(y_train_res)[1])\n",
    "print(\"Minority class in SMOTE training technique with hold-out testing:\", Counter(y_train_res)[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Type-2 in test set : %d\" % (y_test.sum()- len(y_test)))\n",
    "print('*' * 20)\n",
    "res_smote = {}\n",
    "acc_smote = {}\n",
    "\n",
    "for algo in MLalgos:\n",
    "    print(type(MLalgos[algo]).__name__)\n",
    "    y_pred = MLalgos[algo].fit(X_train_res, y_train_res).predict(X_test)\n",
    "    pred_count = (y_pred.sum() - len(y_pred) )\n",
    "    print(\"Predicted minority Class Type [2]:\",pred_count)\n",
    "    res_smote[algo] = pred_count\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    acc_smote[algo] = acc\n",
    "    print(\"Accuracy:{:.2f}\".format(acc))\n",
    "    print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting predicted values using SMOTE technique using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "width = 0.35\n",
    "\n",
    "algorithms = list(MLalgos.keys()) + ['Test-Set']\n",
    "predicted_values = list(bias_calculated.values()) + [test_neg]\n",
    "predicted_values1= list(res_smote.values()) + [test_neg]\n",
    "y_pos = np.arange(len(algorithms))\n",
    "p1 = ax.bar(algorithms, predicted_values, width, align='center', \n",
    "            color=['maroon', 'maroon','maroon','maroon','yellow'])\n",
    "\n",
    "p2 = ax.bar(y_pos+width, predicted_values1, width, align='center', \n",
    "            color=['red', 'red','red','red','yellow'])\n",
    "\n",
    "ax.legend((p1[1], p2[1]), ('Original Hold-out', 'Upsampled Hold-out'))\n",
    "\n",
    "plt.ylabel('Minority Count')\n",
    "plt.title('Upsampling Count')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes\n",
    "\n",
    "- Minority class values have been predicted really well with this technique\n",
    "- The Minority Class values have been upsampled to match with the actual test data.\n",
    "- The accuracy has been pretty much maintained with this technique\n",
    "- KNN, Decision Tree and Logistic regression shows a drastic change in the values produced., However Gradient Boosting has litte or no effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "width = 0.35\n",
    "\n",
    "algorithms = list(MLalgos.keys())\n",
    "predicted_values = list(accuracy_calculated.values())\n",
    "predicted_values1= list(acc_smote.values())\n",
    "y_pos = np.arange(len(algorithms))\n",
    "p1 = ax.bar(algorithms, predicted_values, width, align='center',  color=['maroon', 'maroon','maroon','maroon'])\n",
    "\n",
    "p2 = ax.bar(y_pos+width, predicted_values1, width, align='center', \n",
    "            color=['red', 'red','red','red'])\n",
    "\n",
    "ax.legend((p1[1], p2[1]), ('Accuracy Hold-out', 'Accuracy Upsampled'))\n",
    "\n",
    "plt.ylabel('Minority Count')\n",
    "plt.title('Upsampling Count')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "- The Accuracy metrics shows no effect in case of Gradient Boosting while it show little effect on KNN, Decision Tree and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\">\n",
    "Testing the impact of this strategy on another dataset, and discussing the effectiveness of the strategy on this second dataset.  \n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_pd = pd.read_csv('diabetes.csv')\n",
    "diabetes_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes_pd.pop('neg_pos').values\n",
    "X = diabetes_pd.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Dataset\")\n",
    "print(\"Majority class:\",len(y) - y.sum())\n",
    "print(\"Minority class:\",y.sum())\n",
    "print(\"Minority class: {:.2f}%\".format(y.sum()/len(y)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "bias_diabetes = {}\n",
    "acc_diabetes = {}\n",
    "\n",
    "print(\"Diabetes positive in test set : %d\" % (y_test.sum()))\n",
    "for algo in MLalgos:\n",
    "    y_pred = MLalgos[algo].fit(X_train, y_train).predict(X_test)\n",
    "    pred_count = (y_pred.sum())\n",
    "    bias_diabetes[algo] = pred_count\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    acc_diabetes[algo] = acc\n",
    "  \n",
    "\n",
    "    print(\"{:22} Pred. Diabetes positive: {:d} Accuracy: {:.2f}\".\n",
    "          format(type(MLalgos[algo]).__name__, pred_count,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "algorithms = list(MLalgos.keys()) + ['Test-Set']\n",
    "predicted_values = list(bias_diabetes.values()) + [y_train.sum()]\n",
    "   \n",
    "fig = plt.figure(figsize = (8, 8)) \n",
    "  \n",
    "# creating the bar plot \n",
    "plt.bar(algorithms, predicted_values, color =['maroon','maroon','maroon','maroon','yellow']  ,\n",
    "        width = 0.5)   \n",
    "plt.xlabel(\"CLASSIFICATION MODELS\") \n",
    "plt.ylabel(\"TYPE 2 COUNT IN TEST\") \n",
    "plt.title(\"HOLD-OUT TESTING BIAS PLOT\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=20, sampling_strategy = 0.7)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train), len(y_train_res)\n",
    "y_train.sum(), y_train_res.sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Majority class in previous training technique with hold-out testing:\",Counter(y_train)[0])\n",
    "print(\"Minority class in previous training technique with hold-out testing:\", Counter(y_train)[1])\n",
    "print(\"Majority class in SMOTE technique with hold-out testing:\",Counter(y_train_res)[0])\n",
    "print(\"Minority class in SMOTE training technique with hold-out testing:\", Counter(y_train_res)[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Type-2 in test set : %d\" % (y_test.sum()))\n",
    "print('*' * 20)\n",
    "res_smote = {}\n",
    "acc_smote = {}\n",
    "\n",
    "for algo in MLalgos:\n",
    "    print(type(MLalgos[algo]).__name__)\n",
    "    y_pred = MLalgos[algo].fit(X_train_res, y_train_res).predict(X_test)\n",
    "    pred_count = (y_pred.sum() )\n",
    "    print(\"Predicted minority Class Type [2]:\",pred_count)\n",
    "    res_smote[algo] = pred_count\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    acc_smote[algo] = acc\n",
    "    print(\"Accuracy:{:.2f}\".format(acc))\n",
    "    print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "width = 0.35\n",
    "\n",
    "algorithms = list(MLalgos.keys()) + ['Test-Set']\n",
    "predicted_values = list(bias_diabetes.values()) + [y_test.sum()]\n",
    "predicted_values1= list(res_smote.values()) + [y_test.sum()]\n",
    "y_pos = np.arange(len(algorithms))\n",
    "p1 = ax.bar(algorithms, predicted_values, width, align='center', \n",
    "            color=['maroon', 'maroon','maroon','maroon','yellow'])\n",
    "\n",
    "p2 = ax.bar(y_pos+width, predicted_values1, width, align='center', \n",
    "            color=['red', 'red','red','red','yellow'])\n",
    "\n",
    "ax.legend((p1[1], p2[1]), ('Original Hold-out', 'Upsampled Hold-out'))\n",
    "\n",
    "plt.ylabel('Minority Count')\n",
    "plt.title('Upsampling Count')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "width = 0.35\n",
    "\n",
    "algorithms = list(MLalgos.keys())\n",
    "predicted_values = list(acc_diabetes.values())\n",
    "predicted_values1= list(acc_smote.values())\n",
    "y_pos = np.arange(len(algorithms))\n",
    "p1 = ax.bar(algorithms, predicted_values, width, align='center',  color=['maroon', 'maroon','maroon','maroon'])\n",
    "\n",
    "p2 = ax.bar(y_pos+width, predicted_values1, width, align='center', \n",
    "            color=['red', 'red','red','red'])\n",
    "\n",
    "ax.legend((p1[1], p2[1]), ('Accuracy Hold-out', 'Accuracy Upsampled'))\n",
    "\n",
    "plt.ylabel('Minority Count')\n",
    "plt.title('Upsampling Count')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "- After applying, the Bias of Decision Tree, Logistic Regression and Gradient Boosting has been decreased significantly. \n",
    "- There is no significant changes in accuracy, except there is a minor change in DT.\n",
    "- Logistic Regression seems to be more efficient in terms of low Bias and High accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
